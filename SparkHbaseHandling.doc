 <!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-spark -->
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-spark</artifactId>
    <version>2.0.0-alpha-1</version>
</dependency>

 
 val conf = HBaseConfiguration.create()
    val hbaseContext = new HBaseContext(sc, conf)
    val ColumnNames = schemaFields.split(",")
    hbaseContext.bulkPut[String](streamingRDD,
      TableName.valueOf(tableName),
      (putRecord) => {
        var colIndex = 0
        val put = new Put(Bytes.toBytes(putRecord.split(";,;")(rkIndex))) // RK value
        for (colName <- ColumnNames) {
          putFunc(put, columnFamily, colName, putRecord.split(";,;")(timeStpIndex), putRecord.split(";,;")(colIndex))
          colIndex = colIndex + 1
        }
        put
      })
      
 def putFunc(put: Put, cf: String, colName: String, tsValue: String, colValue: String) {
    put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(colName),
      tsValue.toLong, Bytes.toBytes(colValue))
 }

  val dfColNames = finalcastDF.columns.toList
    val conf = HBaseConfiguration.create()
    val hbaseContext = new HBaseContext(sc, conf)
    hbaseContext.bulkPut[String](streamingRDD,
      TableName.valueOf(tableName),
      (putRecord) => {
        val put = new Put(Bytes.toBytes(putRecord.split(";,;")(dfColNames.indexOf(rkFieldName)))) // RK value
        for (colName <- dfColNames) {
          putFunc(put, columnFamily, colName, putRecord.split(";,;")(dfColNames.indexOf(timeStpFieldName)), putRecord.split(";,;")(dfColNames.indexOf(colName)))
        }
        put
      })
